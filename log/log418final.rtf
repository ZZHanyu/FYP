{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 (base) taotao@ZhyPro14 FYP % \
                                                                                                                                                  \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 50943 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 256, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 256, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Layer1: Data Preprocess: 0it [00:00, ?it/s]\
 size of tokenizer = 888 type = <class 'list'>it/s]\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 54 type = <class 'list'> 1.36it/s]\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 1345 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 342 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 256 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 35 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 286 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 498 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 708 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 391 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 443 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 50 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 511 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 262 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 617 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 156 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 1167 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 427 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 248 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 518 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 354 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 2449 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 411 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 201 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 562 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 834 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 880 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 493 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 87 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 1517 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
\
 size of tokenizer = 325 type = <class 'list'>\
 and embed_dim = 300 type = <class 'numpy.ndarray'>\
\
Processing in a single chunk...: 31it [00:00, 38.94it/s]\
Layer1: Data Preprocess: 0it [00:00, ?it/s]\
Traceback (most recent call last):\
  File "/opt/anaconda3/lib/python3.11/runpy.py", line 198, in _run_module_as_main\
    return _run_code(code, main_globals, None,\
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\
  File "/opt/anaconda3/lib/python3.11/runpy.py", line 88, in _run_code\
    exec(code, run_globals)\
  File "/Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py", line 39, in <module>\
    cli.main()\
  File "/Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 430, in main\
    run()\
  File "/Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 284, in run_file\
    runpy.run_path(target, run_name="__main__")\
  File "/Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 321, in run_path\
    return _run_module_code(code, init_globals, run_name,\
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\
  File "/Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 135, in _run_module_code\
    _run_code(code, mod_globals, init_globals,\
  File "/Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 124, in _run_code\
    exec(code, run_globals)\
  File "/Users/taotao/Documents/GitHub/FYP/classifier.py", line 90, in <module>\
    main_progress.forward()\
  File "/Users/taotao/Documents/GitHub/FYP/classifier.py", line 65, in forward\
    self._data_preprocess()\
  File "/Users/taotao/Documents/GitHub/FYP/classifier.py", line 57, in _data_preprocess\
    data_handler.run()\
  File "/Users/taotao/Documents/GitHub/FYP/utils/preprocess.py", line 123, in run\
    print(f"\\n size of tokenizer = \{len(text_merge)\} type = \{type(text_merge)\}\\n and embed_dim = \{len(text_merge[0])\} type = \{type(text_merge[0])\}\\n")\
                                                                                                  ^^^^^^^^^^^^^^^^^^\
TypeError: object of type 'int' has no len()\
(base) taotao@ZhyPro14 FYP % \
                                                                                                                                                                                          \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bund\
led/libs/debugpy/adapter/../../debugpy/launcher 51058 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Processing in a single chunk...: 97it [00:00, 101.89it/s]\
Processing in a single chunk...: 49it [00:00, 76.54it/s]                                                                                         /Users/taotao/Documents/GitHub/FYP/utils/network.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\
  feature = torch.tensor(batch[data_idx][0], dtype=torch.float32, device=device, requires_grad=True)\
Batch No. 0:   0%|                                                                                                         | 0/97 [00:00<?, ?it/s]\
Layer1: Data Preprocess: 0it [00:00, ?it/s]\
(base) taotao@ZhyPro14 FYP % \
\
 cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/%                                                                                                                                       \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bund\
led/libs/debugpy/adapter/../../debugpy/launcher 51249 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Processing in a single chunk...: 97it [00:01, 88.67it/s] \
Batch No. 0:   0%|                                                                                                         | 0/97 [00:00<?, ?it/s]\
Layer1: Data Preprocess: 0it [00:01, ?it/s]                                                                                | 0/97 [00:00<?, ?it/s]\
Traceback (most recent call last):\
  File "/opt/anaconda3/lib/python3.11/runpy.py", line 198, in _run_module_as_main\
    return _run_code(code, main_globals, None,\
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\
  File "/opt/anaconda3/lib/python3.11/runpy.py", line 88, in _run_code\
    exec(code, run_globals)\
  File "/Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py", line 39, in <module>\
    cli.main()\
  File "/Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 430, in main\
    run()\
  File "/Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 284, in run_file\
    runpy.run_path(target, run_name="__main__")\
  File "/Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 321, in run_path\
    return _run_module_code(code, init_globals, run_name,\
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\
  File "/Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 135, in _run_module_code\
    _run_code(code, mod_globals, init_globals,\
  File "/Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 124, in _run_code\
    exec(code, run_globals)\
  File "/Users/taotao/Documents/GitHub/FYP/classifier.py", line 90, in <module>\
    main_progress.forward()\
  File "/Users/taotao/Documents/GitHub/FYP/classifier.py", line 65, in forward\
    self._data_preprocess()\
  File "/Users/taotao/Documents/GitHub/FYP/classifier.py", line 57, in _data_preprocess\
    data_handler.run()\
  File "/Users/taotao/Documents/GitHub/FYP/utils/preprocess.py", line 128, in run\
    self._LSTM_NetWork.start(chunk_tokenized, index)\
  File "/Users/taotao/Documents/GitHub/FYP/utils/network.py", line 111, in start\
    if idx <= self.args.max_epochs:\
        ^^^^^^^^^^^^^^^^^^^^^^^\
  File "/Users/taotao/Documents/GitHub/FYP/utils/network.py", line 56, in _train\
    # feature = torch.tensor(batch[data_idx][0], dtype=torch.float32, device=device, requires_grad=True)\
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\
ValueError: only one element tensors can be converted to Python scalars\
(base) taotao@ZhyPro14 FYP % \
                                                                                                                                                  \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 51984 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Processing in a single chunk...: 97it [00:01, 91.28it/s] \
Batch No. 0:   0%|                                                                                                         | 0/97 [00:00<?, ?it/s]\
Layer1: Data Preprocess: 0it [00:01, ?it/s]                                                                                | 0/97 [00:00<?, ?it/s]\
(base) taotao@ZhyPro14 FYP % \
                                                                                                                                                  \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 52115 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Processing in a single chunk...: 97it [00:01, 83.21it/s]\
Batch No. 0:   0%|                                                                                                         | 0/97 [00:00<?, ?it/s]\
Layer1: Data Preprocess: 0it [00:01, ?it/s]                                                                                | 0/97 [00:00<?, ?it/s]\
(base) taotao@ZhyPro14 FYP % \
                                                                                                                                                  \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 52294 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Processing in a single chunk...: 97it [00:01, 90.34it/s]\
Batch No. 0:   0%|                                                                                                         | 0/97 [00:00<?, ?it/s]\
Layer1: Data Preprocess: 0it [00:01, ?it/s]                                                                                | 0/97 [00:00<?, ?it/s]\
^[[A^[[A%                                                                                                                                         \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-python.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 52115 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 53248 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Processing in a single chunk...: 97it [00:01, 87.68it/s] \
Batch No. 0:   0%|                                                                                                         | 0/97 [00:00<?, ?it/s]\
Layer1: Data Preprocess: 0it [00:01, ?it/s]                                                                                | 0/97 [00:00<?, ?it/s]\
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 53367 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Processing in a single chunk...: 97it [00:01, 93.25it/s] \
Batch No. 0:   0%|                                                                                                         | 0/97 [00:00<?, ?it/s]\
Layer1: Data Preprocess: 0it [00:01, ?it/s]                                                                                | 0/97 [00:00<?, ?it/s]\
(base) taotao@ZhyPro14 FYP % \
\
 cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/%                                                                                               \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 53672 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Processing in a single chunk...: 97it [00:01, 90.78it/s] \
Layer1: Data Preprocess: 0it [00:01, ?it/s]1, 143.00it/s]\
(base) taotao@ZhyPro14 FYP % \
                                                                                                                                                  \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 53748 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Processing in a single chunk...: 0it [00:00, ?it/s]\
Layer1: Data Preprocess: 0it [00:00, ?it/s], ?it/s]\
(base) taotao@ZhyPro14 FYP % \
                                                                                                                                                  \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 53815 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Processing in a single chunk...: 0it [00:00, ?it/s]\
Layer1: Data Preprocess: 0it [00:00, ?it/s], ?it/s]\
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 54043 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Processing in a single chunk...: 0it [00:00, ?it/s]\
Layer1: Data Preprocess: 0it [00:00, ?it/s], ?it/s]\
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 54230 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Processing in a single chunk...: 0it [00:00, ?it/s]\
Layer1: Data Preprocess: 0it [00:00, ?it/s], ?it/s]\
(base) taotao@ZhyPro14 FYP % \
                                                                                                                                                  \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 54337 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Layer1: Data Preprocess: 0it [00:00, ?it/s]        /Users/taotao/Documents/GitHub/FYP/utils/preprocess.py:92: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\
  return torch.tensor(sentences)\
Processing in a single chunk...: 0it [00:00, ?it/s]\
Layer1: Data Preprocess: 0it [00:00, ?it/s]\
(base) taotao@ZhyPro14 FYP % \
                                                                                                                                                  \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 54439 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Processing in a single chunk...: 0it [00:00, ?it/s]\
Layer1: Data Preprocess: 0it [00:00, ?it/s], ?it/s]\
(base) taotao@ZhyPro14 FYP % \
                                                                                                                                                  \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 54489 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Layer1: Data Preprocess: 0it [00:00, ?it/s]        /Users/taotao/Documents/GitHub/FYP/utils/preprocess.py:92: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\
  return torch.tensor(sentences)\
/Users/taotao/Documents/GitHub/FYP/utils/preprocess.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\
  text_merge = torch.tensor(self._words_embeddings(text_merge))\
Processing in a single chunk...: 97it [00:02, 41.05it/s]\
Batch No. 0:   0%|                                                                                                         | 0/97 [00:00<?, ?it/s]\
Layer1: Data Preprocess: 0it [00:02, ?it/s]                                                                                | 0/97 [00:00<?, ?it/s]\
(base) taotao@ZhyPro14 FYP % \
                                                                                                                                                  \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 54568 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Layer1: Data Preprocess: 0it [00:00, ?it/s]        /Users/taotao/Documents/GitHub/FYP/utils/preprocess.py:92: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\
  return torch.tensor(sentences)\
/Users/taotao/Documents/GitHub/FYP/utils/preprocess.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\
  text_merge = torch.tensor(self._words_embeddings(text_merge), requires_grad=True)\
Processing in a single chunk...: 97it [00:02, 44.65it/s]\
Batch No. 0:   0%|                                                                                                         | 0/97 [00:00<?, ?it/s]\
Layer1: Data Preprocess: 0it [00:02, ?it/s]                                                                                | 0/97 [00:00<?, ?it/s]\
(base) taotao@ZhyPro14 FYP % \
                                                                                                                                                  \
(base) taotao@ZhyPro14 FYP % \
(base) taotao@ZhyPro14 FYP %  cd /Users/taotao/Documents/GitHub/FYP ; /usr/bin/env /opt/anaconda3/bin/python /Users/taotao/.vscode/extensions/ms-p\
ython.debugpy-2024.4.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher 54629 -- /Users/taotao/Documents/GitHub/FYP/classifier.py \
\
\
 Devices selected = mps \
\
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \
The tokenizer class you load from this checkpoint is 'BertTokenizer'. \
The class this function is called from is 'PreTrainedTokenizerFast'.\
\
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)\
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>\
\
Loading models from gensim, name: fasttext-wiki-news-subwords-300 ....\
\
** Load Successfully!\
\
Layer1: Data Preprocess: 0it [00:00, ?it/s]        /Users/taotao/Documents/GitHub/FYP/utils/preprocess.py:92: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\
  sentences[idx] = self.embedding_model.get_vector(sentences[idx], norm=True).astype(np.float32)\
/Users/taotao/Documents/GitHub/FYP/utils/preprocess.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\
  # temp = np.pad(encoder_text['input_ids'], ((0,0), (0, 512 - encoder_text['input_ids'].shape[1])), mode='constant')\
Processing in a single chunk...: 97it [00:02, 42.78it/s]\
Processing in a single chunk...: 89it [00:02, 62.17it/s]                                                                                         (mpsFileLoc): /AppleInternal/Library/BuildRoots/ce725a5f-c761-11ee-a4ec-b6ef2fd8d87b/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm:39:0: error: 'mps.matmul' op contracting dimensions differ 300 & 8\
(mpsFileLoc): /AppleInternal/Library/BuildRoots/ce725a5f-c761-11ee-a4ec-b6ef2fd8d87b/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm:39:0: note: see current operation: %52 = "mps.matmul"(%arg16, %51) <\{transpose_lhs = false, transpose_rhs = false\}> : (tensor<888x1x300xf32>, tensor<8x1024xf32>) -> tensor<888x1x1024xf32>\
/AppleInternal/Library/BuildRoots/ce725a5f-c761-11ee-a4ec-b6ef2fd8d87b/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Headers/Project/MPSGraphExecutable_Project.h:142: failed assertion `Error: executable initialization failed.'\
/opt/anaconda3/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\
  warnings.warn('resource_tracker: There appear to be %d '\
(base) taotao@ZhyPro14 FYP % }