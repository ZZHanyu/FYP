INFO:root:
 ======== Start Log Recording :04-17 16:05 ========

INFO:root:
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>

INFO:root:
 ** Round 0 : Batch size = 23 , avg loss = 9.785256523072071

INFO:root:-->name: normalization.weight -->grad_requirs: True --weight 0.990689754486084 -->grad_value: 5860666880.0 

INFO:root:-->name: normalization.bias -->grad_requirs: True --weight -0.00244910828769207 -->grad_value: -13254159360.0 

INFO:root:-->name: lstm.weight_ih_l0 -->grad_requirs: True --weight -0.00037122672074474394 -->grad_value: 344527136.0 

INFO:root:-->name: lstm.weight_hh_l0 -->grad_requirs: True --weight -8.370456635020673e-05 -->grad_value: 11879478.0 

INFO:root:-->name: lstm.bias_ih_l0 -->grad_requirs: True --weight -0.002350514754652977 -->grad_value: 2139925120.0 

INFO:root:-->name: lstm.bias_hh_l0 -->grad_requirs: True --weight 0.002806557808071375 -->grad_value: 2139925120.0 

INFO:root:-->name: lstm.weight_ih_l0_reverse -->grad_requirs: True --weight 0.000950950721744448 -->grad_value: 58525796.0 

INFO:root:-->name: lstm.weight_hh_l0_reverse -->grad_requirs: True --weight -8.454896305920556e-05 -->grad_value: -2284.474609375 

INFO:root:-->name: lstm.bias_ih_l0_reverse -->grad_requirs: True --weight -0.0006508518126793206 -->grad_value: 227417568.0 

INFO:root:-->name: lstm.bias_hh_l0_reverse -->grad_requirs: True --weight -1.8017599359154701e-06 -->grad_value: 227417568.0 

INFO:root:-->name: lstm.weight_ih_l1 -->grad_requirs: True --weight -0.00013393416884355247 -->grad_value: 6320104.0 

INFO:root:-->name: lstm.weight_hh_l1 -->grad_requirs: True --weight 2.2312859073281288e-05 -->grad_value: -29524938.0 

INFO:root:-->name: lstm.bias_ih_l1 -->grad_requirs: True --weight -0.0001684301532804966 -->grad_value: 1685165312.0 

INFO:root:-->name: lstm.bias_hh_l1 -->grad_requirs: True --weight -0.0007127842982299626 -->grad_value: 1685165312.0 

INFO:root:-->name: lstm.weight_ih_l1_reverse -->grad_requirs: True --weight 9.273197065340355e-05 -->grad_value: 367082.71875 

INFO:root:-->name: lstm.weight_hh_l1_reverse -->grad_requirs: True --weight -0.00022731299395672977 -->grad_value: 0.0 

INFO:root:-->name: lstm.bias_ih_l1_reverse -->grad_requirs: True --weight -0.003364973235875368 -->grad_value: 114019256.0 

INFO:root:-->name: lstm.bias_hh_l1_reverse -->grad_requirs: True --weight -0.0037168716080486774 -->grad_value: 114019256.0 

INFO:root:-->name: linears.0.weight -->grad_requirs: True --weight 0.00021088884386699647 -->grad_value: -182533504.0 

INFO:root:-->name: linears.0.bias -->grad_requirs: True --weight -0.00019251671619713306 -->grad_value: 12944734208.0 

INFO:root:-->name: linears.3.weight -->grad_requirs: True --weight 0.008361845277249813 -->grad_value: 86506823680.0 

INFO:root:-->name: linears.3.bias -->grad_requirs: True --weight -0.03170636668801308 -->grad_value: -1684447035392.0 

INFO:root:
 ** Round 1 : Batch size = 24 , avg loss = 5.199143001363457

INFO:root:-->name: normalization.weight -->grad_requirs: True --weight 0.9869192838668823 -->grad_value: 1693482240.0 

INFO:root:-->name: normalization.bias -->grad_requirs: True --weight -0.0003498685546219349 -->grad_value: -1535187456.0 

INFO:root:-->name: lstm.weight_ih_l0 -->grad_requirs: True --weight -6.947398651391268e-05 -->grad_value: -23663248.0 

INFO:root:-->name: lstm.weight_hh_l0 -->grad_requirs: True --weight -0.00017942223348654807 -->grad_value: 609266.125 

INFO:root:-->name: lstm.bias_ih_l0 -->grad_requirs: True --weight -0.0052679735235869884 -->grad_value: 884174336.0 

INFO:root:-->name: lstm.bias_hh_l0 -->grad_requirs: True --weight -0.00011090090265497565 -->grad_value: 884174336.0 

INFO:root:-->name: lstm.weight_ih_l0_reverse -->grad_requirs: True --weight 0.001363122253678739 -->grad_value: 5100061.0 

INFO:root:-->name: lstm.weight_hh_l0_reverse -->grad_requirs: True --weight -0.00010226468293694779 -->grad_value: -36569.390625 

INFO:root:-->name: lstm.bias_ih_l0_reverse -->grad_requirs: True --weight -0.0024157294537872076 -->grad_value: 46648812.0 

INFO:root:-->name: lstm.bias_hh_l0_reverse -->grad_requirs: True --weight -0.001766679808497429 -->grad_value: 46648812.0 

INFO:root:-->name: lstm.weight_ih_l1 -->grad_requirs: True --weight -0.00019327818881720304 -->grad_value: 764956.4375 

INFO:root:-->name: lstm.weight_hh_l1 -->grad_requirs: True --weight 4.661389539251104e-05 -->grad_value: 339469.5625 

INFO:root:-->name: lstm.bias_ih_l1 -->grad_requirs: True --weight -0.0019702641293406487 -->grad_value: 531106336.0 

INFO:root:-->name: lstm.bias_hh_l1 -->grad_requirs: True --weight -0.0025146191474050283 -->grad_value: 531106336.0 

INFO:root:-->name: lstm.weight_ih_l1_reverse -->grad_requirs: True --weight -1.0117062629433349e-05 -->grad_value: 243766.625 

INFO:root:-->name: lstm.weight_hh_l1_reverse -->grad_requirs: True --weight -0.00022731299395672977 -->grad_value: 0.0 

INFO:root:-->name: lstm.bias_ih_l1_reverse -->grad_requirs: True --weight -0.004773646593093872 -->grad_value: 149699584.0 

INFO:root:-->name: lstm.bias_hh_l1_reverse -->grad_requirs: True --weight -0.005125544965267181 -->grad_value: 149699584.0 

INFO:root:-->name: linears.0.weight -->grad_requirs: True --weight 0.0003494151751510799 -->grad_value: 16099934.0 

INFO:root:-->name: linears.0.bias -->grad_requirs: True --weight -0.0010831011459231377 -->grad_value: -8342683648.0 

INFO:root:-->name: linears.3.weight -->grad_requirs: True --weight 0.009503945708274841 -->grad_value: -25358102528.0 

INFO:root:-->name: linears.3.bias -->grad_requirs: True --weight -0.023471543565392494 -->grad_value: -1118989451264.0 

INFO:root:
 ** Round 2 : Batch size = 25 , avg loss = 0.9939599376916886

INFO:root:-->name: normalization.weight -->grad_requirs: True --weight 0.9849842190742493 -->grad_value: 0.18155726790428162 

INFO:root:-->name: normalization.bias -->grad_requirs: True --weight 0.0008294181898236275 -->grad_value: 0.22822155058383942 

INFO:root:-->name: lstm.weight_ih_l0 -->grad_requirs: True --weight -4.104396793991327e-05 -->grad_value: 0.002676185220479965 

INFO:root:-->name: lstm.weight_hh_l0 -->grad_requirs: True --weight -0.00017254491103813052 -->grad_value: 5.207341473578708e-06 

INFO:root:-->name: lstm.bias_ih_l0 -->grad_requirs: True --weight -0.00599631667137146 -->grad_value: 0.028492525219917297 

INFO:root:-->name: lstm.bias_hh_l0 -->grad_requirs: True --weight -0.0008392431773245335 -->grad_value: 0.028492525219917297 

INFO:root:-->name: lstm.weight_ih_l0_reverse -->grad_requirs: True --weight 0.0012876807013526559 -->grad_value: 0.0009857129771262407 

INFO:root:-->name: lstm.weight_hh_l0_reverse -->grad_requirs: True --weight -0.00010957060294458643 -->grad_value: -3.9016545088088606e-06 

INFO:root:-->name: lstm.bias_ih_l0_reverse -->grad_requirs: True --weight -0.002490011975169182 -->grad_value: -0.0017347687389701605 

INFO:root:-->name: lstm.bias_hh_l0_reverse -->grad_requirs: True --weight -0.0018409625627100468 -->grad_value: -0.0017347687389701605 

INFO:root:-->name: lstm.weight_ih_l1 -->grad_requirs: True --weight -0.00020872120512649417 -->grad_value: -1.8217986507806927e-05 

INFO:root:-->name: lstm.weight_hh_l1 -->grad_requirs: True --weight 2.4460561689920723e-05 -->grad_value: -4.327212445787154e-05 

INFO:root:-->name: lstm.bias_ih_l1 -->grad_requirs: True --weight -0.0030905476305633783 -->grad_value: 0.02405947633087635 

INFO:root:-->name: lstm.bias_hh_l1 -->grad_requirs: True --weight -0.0036349024157971144 -->grad_value: 0.02405947633087635 

INFO:root:-->name: lstm.weight_ih_l1_reverse -->grad_requirs: True --weight -2.2862968762638047e-05 -->grad_value: -3.7336908462748397e-06 

INFO:root:-->name: lstm.weight_hh_l1_reverse -->grad_requirs: True --weight -0.00022731299395672977 -->grad_value: 0.0 

INFO:root:-->name: lstm.bias_ih_l1_reverse -->grad_requirs: True --weight -0.005024716258049011 -->grad_value: 0.008581900037825108 

INFO:root:-->name: lstm.bias_hh_l1_reverse -->grad_requirs: True --weight -0.005376614164561033 -->grad_value: 0.008581900037825108 

INFO:root:-->name: linears.0.weight -->grad_requirs: True --weight 0.00038138520903885365 -->grad_value: 0.0057844859547913074 

INFO:root:-->name: linears.0.bias -->grad_requirs: True --weight -0.0016243797726929188 -->grad_value: -1.5498017072677612 

INFO:root:-->name: linears.3.weight -->grad_requirs: True --weight 0.011002854444086552 -->grad_value: -2.421452045440674 

INFO:root:-->name: linears.3.bias -->grad_requirs: True --weight -0.021349674090743065 -->grad_value: -70.83333587646484 

INFO:root:
 ** Round 3 : Batch size = 25 , avg loss = 0.9233382171392441

INFO:root:-->name: normalization.weight -->grad_requirs: True --weight 0.9849193096160889 -->grad_value: 0.15889135003089905 

INFO:root:-->name: normalization.bias -->grad_requirs: True --weight 0.0009216475882567465 -->grad_value: 0.2295335829257965 

INFO:root:-->name: lstm.weight_ih_l0 -->grad_requirs: True --weight -3.8820202462375164e-05 -->grad_value: 0.0006639013299718499 

INFO:root:-->name: lstm.weight_hh_l0 -->grad_requirs: True --weight -0.00017200695583596826 -->grad_value: -3.285933780716732e-05 

INFO:root:-->name: lstm.bias_ih_l0 -->grad_requirs: True --weight -0.006053278222680092 -->grad_value: 0.006767453625798225 

INFO:root:-->name: lstm.bias_hh_l0 -->grad_requirs: True --weight -0.0008962057763710618 -->grad_value: 0.006767453625798225 

INFO:root:-->name: lstm.weight_ih_l0_reverse -->grad_requirs: True --weight 0.001281780656427145 -->grad_value: 0.001818454242311418 

INFO:root:-->name: lstm.weight_hh_l0_reverse -->grad_requirs: True --weight -0.00011014198389602825 -->grad_value: -1.8180335246142931e-06 

INFO:root:-->name: lstm.bias_ih_l0_reverse -->grad_requirs: True --weight -0.002495821099728346 -->grad_value: -0.0029695150442421436 

INFO:root:-->name: lstm.bias_hh_l0_reverse -->grad_requirs: True --weight -0.0018467718036845326 -->grad_value: -0.0029695150442421436 

INFO:root:-->name: lstm.weight_ih_l1 -->grad_requirs: True --weight -0.00020992898498661816 -->grad_value: 7.105531949491706e-06 

INFO:root:-->name: lstm.weight_hh_l1 -->grad_requirs: True --weight 2.2727988834958524e-05 -->grad_value: -2.4868531909305602e-05 

INFO:root:-->name: lstm.bias_ih_l1 -->grad_requirs: True --weight -0.0031781625002622604 -->grad_value: 0.00874466635286808 

INFO:root:-->name: lstm.bias_hh_l1 -->grad_requirs: True --weight -0.0037225172854959965 -->grad_value: 0.00874466635286808 

INFO:root:-->name: lstm.weight_ih_l1_reverse -->grad_requirs: True --weight -2.3859789507696405e-05 -->grad_value: -2.8969509457965614e-06 

INFO:root:-->name: lstm.weight_hh_l1_reverse -->grad_requirs: True --weight -0.00022731299395672977 -->grad_value: 0.0 

INFO:root:-->name: lstm.bias_ih_l1_reverse -->grad_requirs: True --weight -0.005044352263212204 -->grad_value: 0.006561314687132835 

INFO:root:-->name: lstm.bias_hh_l1_reverse -->grad_requirs: True --weight -0.005396249238401651 -->grad_value: 0.006561314687132835 

INFO:root:-->name: linears.0.weight -->grad_requirs: True --weight 0.0003962201881222427 -->grad_value: 0.004586155526340008 

INFO:root:-->name: linears.0.bias -->grad_requirs: True --weight -0.0018245987594127655 -->grad_value: -1.0877033472061157 

INFO:root:-->name: linears.3.weight -->grad_requirs: True --weight 0.01127796620130539 -->grad_value: -2.2466416358947754 

INFO:root:-->name: linears.3.bias -->grad_requirs: True --weight -0.02118372544646263 -->grad_value: -49.30906677246094 

INFO:root:
 ** Round 4 : Batch size = 24 , avg loss = 0.9026277322943012

INFO:root:-->name: normalization.weight -->grad_requirs: True --weight 0.984870433807373 -->grad_value: -0.07455075532197952 

INFO:root:-->name: normalization.bias -->grad_requirs: True --weight 0.000929210102185607 -->grad_value: 0.24731720983982086 

INFO:root:-->name: lstm.weight_ih_l0 -->grad_requirs: True --weight -3.863801248371601e-05 -->grad_value: 0.002206344623118639 

INFO:root:-->name: lstm.weight_hh_l0 -->grad_requirs: True --weight -0.00017196289263665676 -->grad_value: -4.086066837771796e-06 

INFO:root:-->name: lstm.bias_ih_l0 -->grad_requirs: True --weight -0.0060579488053917885 -->grad_value: 0.003281182376667857 

INFO:root:-->name: lstm.bias_hh_l0 -->grad_requirs: True --weight -0.0009008761262521148 -->grad_value: 0.003281182376667857 

INFO:root:-->name: lstm.weight_ih_l0_reverse -->grad_requirs: True --weight 0.001281296950764954 -->grad_value: -0.0018123211339116096 

INFO:root:-->name: lstm.weight_hh_l0_reverse -->grad_requirs: True --weight -0.00011018887016689405 -->grad_value: -5.171283191884868e-06 

INFO:root:-->name: lstm.bias_ih_l0_reverse -->grad_requirs: True --weight -0.0024962970055639744 -->grad_value: -0.005673221778124571 

INFO:root:-->name: lstm.bias_hh_l0_reverse -->grad_requirs: True --weight -0.001847248524427414 -->grad_value: -0.005673221778124571 

INFO:root:-->name: lstm.weight_ih_l1 -->grad_requirs: True --weight -0.0002100279671140015 -->grad_value: 1.702172085060738e-06 

INFO:root:-->name: lstm.weight_hh_l1 -->grad_requirs: True --weight 2.2585933038499206e-05 -->grad_value: -1.8630578779266216e-05 

INFO:root:-->name: lstm.bias_ih_l1 -->grad_requirs: True --weight -0.0031853464897722006 -->grad_value: 0.0073388246819376945 

INFO:root:-->name: lstm.bias_hh_l1 -->grad_requirs: True --weight -0.0037297012750059366 -->grad_value: 0.0073388246819376945 

INFO:root:-->name: lstm.weight_ih_l1_reverse -->grad_requirs: True --weight -2.3941545805428177e-05 -->grad_value: 1.3497106010618154e-05 

INFO:root:-->name: lstm.weight_hh_l1_reverse -->grad_requirs: True --weight -0.00022731299395672977 -->grad_value: 0.0 

INFO:root:-->name: lstm.bias_ih_l1_reverse -->grad_requirs: True --weight -0.005045962519943714 -->grad_value: 0.003376877401024103 

INFO:root:-->name: lstm.bias_hh_l1_reverse -->grad_requirs: True --weight -0.0053978594951331615 -->grad_value: 0.003376877401024103 

INFO:root:-->name: linears.0.weight -->grad_requirs: True --weight 0.0003974365827161819 -->grad_value: 0.004321273881942034 

INFO:root:-->name: linears.0.bias -->grad_requirs: True --weight -0.0018410132033750415 -->grad_value: -1.057835340499878 

INFO:root:-->name: linears.3.weight -->grad_requirs: True --weight 0.011300524696707726 -->grad_value: -2.4269213676452637 

INFO:root:-->name: linears.3.bias -->grad_requirs: True --weight -0.0211701188236475 -->grad_value: -44.284950256347656 

INFO:root:
 ** Round 5 : Batch size = 25 , avg loss = 4.595252410471439

INFO:root:-->name: normalization.weight -->grad_requirs: True --weight 0.9758224487304688 -->grad_value: 9348794368.0 

INFO:root:-->name: normalization.bias -->grad_requirs: True --weight 0.0002349198330193758 -->grad_value: 741007168.0 

INFO:root:-->name: lstm.weight_ih_l0 -->grad_requirs: True --weight 0.0005667891819030046 -->grad_value: -27478510.0 

INFO:root:-->name: lstm.weight_hh_l0 -->grad_requirs: True --weight -1.0000934707932174e-05 -->grad_value: -93691.15625 

INFO:root:-->name: lstm.bias_ih_l0 -->grad_requirs: True --weight -0.0076642632484436035 -->grad_value: 47852032.0 

INFO:root:-->name: lstm.bias_hh_l0 -->grad_requirs: True --weight -0.002507190452888608 -->grad_value: 47852032.0 

INFO:root:-->name: lstm.weight_ih_l0_reverse -->grad_requirs: True --weight 0.0030955024994909763 -->grad_value: 130032816.0 

INFO:root:-->name: lstm.weight_hh_l0_reverse -->grad_requirs: True --weight -0.00018189259571954608 -->grad_value: -427438.28125 

INFO:root:-->name: lstm.bias_ih_l0_reverse -->grad_requirs: True --weight -0.005975258536636829 -->grad_value: 37457364.0 

INFO:root:-->name: lstm.bias_hh_l0_reverse -->grad_requirs: True --weight -0.005326210055500269 -->grad_value: 37457364.0 

INFO:root:-->name: lstm.weight_ih_l1 -->grad_requirs: True --weight -0.00019866452203132212 -->grad_value: 108126.4375 

INFO:root:-->name: lstm.weight_hh_l1 -->grad_requirs: True --weight 6.0539241530932486e-05 -->grad_value: 68434.46875 

INFO:root:-->name: lstm.bias_ih_l1 -->grad_requirs: True --weight -0.003658103756606579 -->grad_value: -3796606.0 

INFO:root:-->name: lstm.bias_hh_l1 -->grad_requirs: True --weight -0.004202458541840315 -->grad_value: -3796606.0 

INFO:root:-->name: lstm.weight_ih_l1_reverse -->grad_requirs: True --weight -0.00021181281772442162 -->grad_value: 486840.09375 

INFO:root:-->name: lstm.weight_hh_l1_reverse -->grad_requirs: True --weight -0.00022731299395672977 -->grad_value: 0.0 

INFO:root:-->name: lstm.bias_ih_l1_reverse -->grad_requirs: True --weight -0.009351132437586784 -->grad_value: 125964720.0 

INFO:root:-->name: lstm.bias_hh_l1_reverse -->grad_requirs: True --weight -0.009703028947114944 -->grad_value: 125964720.0 

INFO:root:-->name: linears.0.weight -->grad_requirs: True --weight 0.0003702544781845063 -->grad_value: 2084625.0 

INFO:root:-->name: linears.0.bias -->grad_requirs: True --weight -0.00076223851647228 -->grad_value: -966842240.0 

INFO:root:-->name: linears.3.weight -->grad_requirs: True --weight 0.015708258375525475 -->grad_value: -14688452608.0 

INFO:root:-->name: linears.3.bias -->grad_requirs: True --weight -0.007686781231313944 -->grad_value: -1001525739520.0 

INFO:root:
 ** Round 6 : Batch size = 24 , avg loss = 9.134664913561815

INFO:root:-->name: normalization.weight -->grad_requirs: True --weight 0.9899317026138306 -->grad_value: 4997134336.0 

INFO:root:-->name: normalization.bias -->grad_requirs: True --weight 0.0035340976901352406 -->grad_value: 20020824064.0 

INFO:root:-->name: lstm.weight_ih_l0 -->grad_requirs: True --weight -0.0014490052126348019 -->grad_value: 1657484672.0 

INFO:root:-->name: lstm.weight_hh_l0 -->grad_requirs: True --weight 0.00045485084410756826 -->grad_value: -5843294.0 

INFO:root:-->name: lstm.bias_ih_l0 -->grad_requirs: True --weight -0.015688322484493256 -->grad_value: 3434950656.0 

INFO:root:-->name: lstm.bias_hh_l0 -->grad_requirs: True --weight -0.010531249456107616 -->grad_value: 3434950656.0 

INFO:root:-->name: lstm.weight_ih_l0_reverse -->grad_requirs: True --weight 0.001074732281267643 -->grad_value: 4807964160.0 

INFO:root:-->name: lstm.weight_hh_l0_reverse -->grad_requirs: True --weight 0.0002837959036696702 -->grad_value: -6515540.5 

INFO:root:-->name: lstm.bias_ih_l0_reverse -->grad_requirs: True --weight -0.01410825178027153 -->grad_value: 1324144896.0 

INFO:root:-->name: lstm.bias_hh_l0_reverse -->grad_requirs: True --weight -0.013459202833473682 -->grad_value: 1324144896.0 

INFO:root:-->name: lstm.weight_ih_l1 -->grad_requirs: True --weight -0.0004560222150757909 -->grad_value: -7877023.5 

INFO:root:-->name: lstm.weight_hh_l1 -->grad_requirs: True --weight 0.000360882724635303 -->grad_value: -39611472.0 

INFO:root:-->name: lstm.bias_ih_l1 -->grad_requirs: True --weight -0.017833959311246872 -->grad_value: 3520135680.0 

INFO:root:-->name: lstm.bias_hh_l1 -->grad_requirs: True --weight -0.01837831363081932 -->grad_value: 3520135680.0 

INFO:root:-->name: lstm.weight_ih_l1_reverse -->grad_requirs: True --weight -0.00011142724542878568 -->grad_value: -3138058.0 

INFO:root:-->name: lstm.weight_hh_l1_reverse -->grad_requirs: True --weight -0.00022731299395672977 -->grad_value: 0.0 

INFO:root:-->name: lstm.bias_ih_l1_reverse -->grad_requirs: True --weight -0.01876186951994896 -->grad_value: 537819392.0 

INFO:root:-->name: lstm.bias_hh_l1_reverse -->grad_requirs: True --weight -0.01911376602947712 -->grad_value: 537819392.0 

INFO:root:-->name: linears.0.weight -->grad_requirs: True --weight 0.0010197870433330536 -->grad_value: -461423200.0 

INFO:root:-->name: linears.0.bias -->grad_requirs: True --weight -0.006534358020871878 -->grad_value: 31120801792.0 

INFO:root:-->name: linears.3.weight -->grad_requirs: True --weight 0.010761058889329433 -->grad_value: 312292147200.0 

INFO:root:-->name: linears.3.bias -->grad_requirs: True --weight -0.013165599666535854 -->grad_value: 271715139584.0 

INFO:root:
 ** Round 7 : Batch size = 24 , avg loss = 9.198185639010868

INFO:root:-->name: normalization.weight -->grad_requirs: True --weight 0.9900412559509277 -->grad_value: 52158267392.0 

INFO:root:-->name: normalization.bias -->grad_requirs: True --weight 0.018172545358538628 -->grad_value: -107023327232.0 

INFO:root:-->name: lstm.weight_ih_l0 -->grad_requirs: True --weight -0.0032459802459925413 -->grad_value: 3165894144.0 

INFO:root:-->name: lstm.weight_hh_l0 -->grad_requirs: True --weight 0.0007880894700065255 -->grad_value: 1083080.625 

INFO:root:-->name: lstm.bias_ih_l0 -->grad_requirs: True --weight -0.027663420885801315 -->grad_value: 2516888832.0 

INFO:root:-->name: lstm.bias_hh_l0 -->grad_requirs: True --weight -0.02250634878873825 -->grad_value: 2516888832.0 

INFO:root:-->name: lstm.weight_ih_l0_reverse -->grad_requirs: True --weight 9.641505312174559e-05 -->grad_value: 2683565824.0 

INFO:root:-->name: lstm.weight_hh_l0_reverse -->grad_requirs: True --weight 0.0008572421502321959 -->grad_value: -6117199.0 

INFO:root:-->name: lstm.bias_ih_l0_reverse -->grad_requirs: True --weight -0.01909691095352173 -->grad_value: 1133116928.0 

INFO:root:-->name: lstm.bias_hh_l0_reverse -->grad_requirs: True --weight -0.018447862938046455 -->grad_value: 1133116928.0 

INFO:root:-->name: lstm.weight_ih_l1 -->grad_requirs: True --weight -0.00020370683341752738 -->grad_value: -17525776.0 

INFO:root:-->name: lstm.weight_hh_l1 -->grad_requirs: True --weight 0.0002321906213182956 -->grad_value: 11711266.0 

INFO:root:-->name: lstm.bias_ih_l1 -->grad_requirs: True --weight -0.02788105234503746 -->grad_value: 1170859392.0 

INFO:root:-->name: lstm.bias_hh_l1 -->grad_requirs: True --weight -0.02842540666460991 -->grad_value: 1170859392.0 

INFO:root:-->name: lstm.weight_ih_l1_reverse -->grad_requirs: True --weight 0.0003015052352566272 -->grad_value: -23873322.0 

INFO:root:-->name: lstm.weight_hh_l1_reverse -->grad_requirs: True --weight -0.00022731299395672977 -->grad_value: 0.0 

INFO:root:-->name: lstm.bias_ih_l1_reverse -->grad_requirs: True --weight -0.027061760425567627 -->grad_value: 676189184.0 

INFO:root:-->name: lstm.bias_hh_l1_reverse -->grad_requirs: True --weight -0.027413658797740936 -->grad_value: 676189184.0 

INFO:root:-->name: linears.0.weight -->grad_requirs: True --weight 0.0010853082640096545 -->grad_value: 72094464.0 

INFO:root:-->name: linears.0.bias -->grad_requirs: True --weight -0.011021586135029793 -->grad_value: 12683250688.0 

INFO:root:-->name: linears.3.weight -->grad_requirs: True --weight 0.005515511613339186 -->grad_value: 575801786368.0 

INFO:root:-->name: linears.3.bias -->grad_requirs: True --weight -0.00566919194534421 -->grad_value: -1092114186240.0 

