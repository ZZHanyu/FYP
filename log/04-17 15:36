INFO:root:
 ======== Start Log Recording :04-17 15:36 ========

INFO:root:
 Model Initialization = LSTM(8, 128, num_layers=2, bidirectional=True)
 *Parameters = <bound method Module.parameters of LSTM(8, 128, num_layers=2, bidirectional=True)>

INFO:root:
 ** Round 0 : Batch size = 23 , avg loss = 5.564539531002874

INFO:root:-->name: normalization.weight -->grad_requirs: True --weight 0.9895552396774292 -->grad_value: 6948965376.0 

INFO:root:-->name: normalization.bias -->grad_requirs: True --weight 0.006598145700991154 -->grad_value: 97430380544.0 

INFO:root:-->name: lstm.weight_ih_l0 -->grad_requirs: True --weight 0.0013533070450648665 -->grad_value: 247009760.0 

INFO:root:-->name: lstm.weight_hh_l0 -->grad_requirs: True --weight -0.000434779969509691 -->grad_value: -10951696.0 

INFO:root:-->name: lstm.bias_ih_l0 -->grad_requirs: True --weight -0.004309270530939102 -->grad_value: 2652460032.0 

INFO:root:-->name: lstm.bias_hh_l0 -->grad_requirs: True --weight -0.0024244035594165325 -->grad_value: 2652460032.0 

INFO:root:-->name: lstm.weight_ih_l0_reverse -->grad_requirs: True --weight 0.0006484740879386663 -->grad_value: 150878224.0 

INFO:root:-->name: lstm.weight_hh_l0_reverse -->grad_requirs: True --weight -0.00019736203830689192 -->grad_value: -4313958.0 

INFO:root:-->name: lstm.bias_ih_l0_reverse -->grad_requirs: True --weight 0.0017144693993031979 -->grad_value: 2063960960.0 

INFO:root:-->name: lstm.bias_hh_l0_reverse -->grad_requirs: True --weight 0.0007044065278023481 -->grad_value: 2063960960.0 

INFO:root:-->name: lstm.weight_ih_l1 -->grad_requirs: True --weight 0.0001834498398238793 -->grad_value: -19771696.0 

INFO:root:-->name: lstm.weight_hh_l1 -->grad_requirs: True --weight -0.0002320242638234049 -->grad_value: -85888040.0 

INFO:root:-->name: lstm.bias_ih_l1 -->grad_requirs: True --weight -0.0010628507006913424 -->grad_value: 6002152448.0 

INFO:root:-->name: lstm.bias_hh_l1 -->grad_requirs: True --weight -0.0036729963030666113 -->grad_value: 6002152448.0 

INFO:root:-->name: lstm.weight_ih_l1_reverse -->grad_requirs: True --weight 0.00022986608382780105 -->grad_value: -968689.5 

INFO:root:-->name: lstm.weight_hh_l1_reverse -->grad_requirs: True --weight -1.476364559493959e-05 -->grad_value: 0.0 

INFO:root:-->name: lstm.bias_ih_l1_reverse -->grad_requirs: True --weight -0.0006984479841776192 -->grad_value: 419583168.0 

INFO:root:-->name: lstm.bias_hh_l1_reverse -->grad_requirs: True --weight 0.0003760139225050807 -->grad_value: 419583168.0 

INFO:root:-->name: linear.weight -->grad_requirs: True --weight -0.0017378437332808971 -->grad_value: -5865206784.0 

INFO:root:-->name: linear.bias -->grad_requirs: True --weight -0.000148697174154222 -->grad_value: 1072203431936.0 

INFO:root:
 ** Round 1 : Batch size = 24 , avg loss = 17.422067859986175

INFO:root:-->name: normalization.weight -->grad_requirs: True --weight 0.9839185476303101 -->grad_value: 15459201024.0 

INFO:root:-->name: normalization.bias -->grad_requirs: True --weight 0.0021806261502206326 -->grad_value: 6551910400.0 

INFO:root:-->name: lstm.weight_ih_l0 -->grad_requirs: True --weight 0.0011618923163041472 -->grad_value: 32900694.0 

INFO:root:-->name: lstm.weight_hh_l0 -->grad_requirs: True --weight -0.00017125345766544342 -->grad_value: 6863215.5 

INFO:root:-->name: lstm.bias_ih_l0 -->grad_requirs: True --weight -0.008651072159409523 -->grad_value: -1578286592.0 

INFO:root:-->name: lstm.bias_hh_l0 -->grad_requirs: True --weight -0.006766205187886953 -->grad_value: -1578286592.0 

INFO:root:-->name: lstm.weight_ih_l0_reverse -->grad_requirs: True --weight 0.0005412183236330748 -->grad_value: 118098448.0 

INFO:root:-->name: lstm.weight_hh_l0_reverse -->grad_requirs: True --weight -0.00010397049481980503 -->grad_value: -91974.8125 

INFO:root:-->name: lstm.bias_ih_l0_reverse -->grad_requirs: True --weight -0.001375124673359096 -->grad_value: -551624512.0 

INFO:root:-->name: lstm.bias_hh_l0_reverse -->grad_requirs: True --weight -0.0023851878941059113 -->grad_value: -551624512.0 

INFO:root:-->name: lstm.weight_ih_l1 -->grad_requirs: True --weight 0.00035609136102721095 -->grad_value: -7783467.0 

INFO:root:-->name: lstm.weight_hh_l1 -->grad_requirs: True --weight -0.0002562027657404542 -->grad_value: -3952075.5 

INFO:root:-->name: lstm.bias_ih_l1 -->grad_requirs: True --weight -0.003354923101142049 -->grad_value: 1598041472.0 

INFO:root:-->name: lstm.bias_hh_l1 -->grad_requirs: True --weight -0.005965069402009249 -->grad_value: 1598041472.0 

INFO:root:-->name: lstm.weight_ih_l1_reverse -->grad_requirs: True --weight 0.00048292975407093763 -->grad_value: 1104613.5 

INFO:root:-->name: lstm.weight_hh_l1_reverse -->grad_requirs: True --weight -1.476364559493959e-05 -->grad_value: 0.0 

INFO:root:-->name: lstm.bias_ih_l1_reverse -->grad_requirs: True --weight -0.0044680493883788586 -->grad_value: -227460672.0 

INFO:root:-->name: lstm.bias_hh_l1_reverse -->grad_requirs: True --weight -0.0033935867249965668 -->grad_value: -227460672.0 

INFO:root:-->name: linear.weight -->grad_requirs: True --weight -0.0016550598666071892 -->grad_value: 6049758208.0 

INFO:root:-->name: linear.bias -->grad_requirs: True --weight 0.005842769518494606 -->grad_value: -4260218535936.0 

INFO:root:
 ** Round 2 : Batch size = 25 , avg loss = 0.7093726980686188

INFO:root:-->name: normalization.weight -->grad_requirs: True --weight 0.9796344041824341 -->grad_value: 0.09739324450492859 

INFO:root:-->name: normalization.bias -->grad_requirs: True --weight -0.00018445029854774475 -->grad_value: 0.23007410764694214 

INFO:root:-->name: lstm.weight_ih_l0 -->grad_requirs: True --weight 0.0012366429436951876 -->grad_value: -0.0021226329263299704 

INFO:root:-->name: lstm.weight_hh_l0 -->grad_requirs: True --weight 1.5149969840422273e-05 -->grad_value: 1.7745684090186842e-05 

INFO:root:-->name: lstm.bias_ih_l0 -->grad_requirs: True --weight -0.01145385392010212 -->grad_value: -0.00131236354354769 

INFO:root:-->name: lstm.bias_hh_l0 -->grad_requirs: True --weight -0.009568987414240837 -->grad_value: -0.00131236354354769 

INFO:root:-->name: lstm.weight_ih_l0_reverse -->grad_requirs: True --weight 0.0005504884757101536 -->grad_value: 0.0010170831810683012 

INFO:root:-->name: lstm.weight_hh_l0_reverse -->grad_requirs: True --weight -2.712001878535375e-05 -->grad_value: 1.3810133623337606e-06 

INFO:root:-->name: lstm.bias_ih_l0_reverse -->grad_requirs: True --weight -0.0025985147804021835 -->grad_value: -1.8081162124872208e-05 

INFO:root:-->name: lstm.bias_hh_l0_reverse -->grad_requirs: True --weight -0.0036085776519030333 -->grad_value: -1.8081162124872208e-05 

INFO:root:-->name: lstm.weight_ih_l1 -->grad_requirs: True --weight 0.00046082522021606565 -->grad_value: 1.9795234038610943e-05 

INFO:root:-->name: lstm.weight_hh_l1 -->grad_requirs: True --weight -0.00030577252618968487 -->grad_value: 4.8545694880886e-05 

INFO:root:-->name: lstm.bias_ih_l1 -->grad_requirs: True --weight -0.004558723419904709 -->grad_value: -0.0065402677282691 

INFO:root:-->name: lstm.bias_hh_l1 -->grad_requirs: True --weight -0.007168870884925127 -->grad_value: -0.0065402677282691 

INFO:root:-->name: lstm.weight_ih_l1_reverse -->grad_requirs: True --weight 0.0005345672834664583 -->grad_value: 6.6176289692521095e-06 

INFO:root:-->name: lstm.weight_hh_l1_reverse -->grad_requirs: True --weight -1.476364559493959e-05 -->grad_value: 0.0 

INFO:root:-->name: lstm.bias_ih_l1_reverse -->grad_requirs: True --weight -0.005131787154823542 -->grad_value: -0.003683016635477543 

INFO:root:-->name: lstm.bias_hh_l1_reverse -->grad_requirs: True --weight -0.004057324957102537 -->grad_value: -0.003683016635477543 

INFO:root:-->name: linear.weight -->grad_requirs: True --weight -0.0014367057010531425 -->grad_value: 0.04714660346508026 

INFO:root:-->name: linear.bias -->grad_requirs: True --weight 0.01745588332414627 -->grad_value: -14.401824951171875 

INFO:root:
 ** Round 3 : Batch size = 25 , avg loss = 0.6885267627239228

INFO:root:-->name: normalization.weight -->grad_requirs: True --weight 0.9792993068695068 -->grad_value: -0.022815998643636703 

INFO:root:-->name: normalization.bias -->grad_requirs: True --weight -0.0003694174811244011 -->grad_value: 0.15771996974945068 

INFO:root:-->name: lstm.weight_ih_l0 -->grad_requirs: True --weight 0.0012424893211573362 -->grad_value: -0.0007568452274426818 

INFO:root:-->name: lstm.weight_hh_l0 -->grad_requirs: True --weight 2.9728325898759067e-05 -->grad_value: 1.7049169400706887e-06 

INFO:root:-->name: lstm.bias_ih_l0 -->grad_requirs: True --weight -0.011673054657876492 -->grad_value: -0.0007434091530740261 

INFO:root:-->name: lstm.bias_hh_l0 -->grad_requirs: True --weight -0.009788187220692635 -->grad_value: -0.0007434091530740261 

INFO:root:-->name: lstm.weight_ih_l0_reverse -->grad_requirs: True --weight 0.0005512135103344917 -->grad_value: 0.0007706157630309463 

INFO:root:-->name: lstm.weight_hh_l0_reverse -->grad_requirs: True --weight -2.110967761836946e-05 -->grad_value: 3.5008167742489604e-07 

INFO:root:-->name: lstm.bias_ih_l0_reverse -->grad_requirs: True --weight -0.0026941944379359484 -->grad_value: -0.0007650722982361913 

INFO:root:-->name: lstm.bias_hh_l0_reverse -->grad_requirs: True --weight -0.003704256610944867 -->grad_value: -0.0007650722982361913 

INFO:root:-->name: lstm.weight_ih_l1 -->grad_requirs: True --weight 0.0004690162604674697 -->grad_value: 4.6542099880753085e-06 

INFO:root:-->name: lstm.weight_hh_l1 -->grad_requirs: True --weight -0.0003096493019256741 -->grad_value: 4.3960102630080655e-05 

INFO:root:-->name: lstm.bias_ih_l1 -->grad_requirs: True --weight -0.004652870818972588 -->grad_value: -0.005400999449193478 

INFO:root:-->name: lstm.bias_hh_l1 -->grad_requirs: True --weight -0.0072630178183317184 -->grad_value: -0.005400999449193478 

INFO:root:-->name: lstm.weight_ih_l1_reverse -->grad_requirs: True --weight 0.0005386058473959565 -->grad_value: 5.628093731502304e-06 

INFO:root:-->name: lstm.weight_hh_l1_reverse -->grad_requirs: True --weight -1.476364559493959e-05 -->grad_value: 0.0 

INFO:root:-->name: lstm.bias_ih_l1_reverse -->grad_requirs: True --weight -0.005183696746826172 -->grad_value: -0.0035244878381490707 

INFO:root:-->name: lstm.bias_hh_l1_reverse -->grad_requirs: True --weight -0.00410923408344388 -->grad_value: -0.0035244878381490707 

INFO:root:-->name: linear.weight -->grad_requirs: True --weight -0.0014196287374943495 -->grad_value: 0.03393789753317833 

INFO:root:-->name: linear.bias -->grad_requirs: True --weight 0.01836412400007248 -->grad_value: -8.788104057312012 

INFO:root:
 ** Round 4 : Batch size = 24 , avg loss = 0.6907303240150213

INFO:root:-->name: normalization.weight -->grad_requirs: True --weight 0.9792718887329102 -->grad_value: 0.006050321273505688 

INFO:root:-->name: normalization.bias -->grad_requirs: True --weight -0.0003845819737762213 -->grad_value: 0.2943447232246399 

INFO:root:-->name: lstm.weight_ih_l0 -->grad_requirs: True --weight 0.0012429687194526196 -->grad_value: -0.0013784883776679635 

INFO:root:-->name: lstm.weight_hh_l0 -->grad_requirs: True --weight 3.0923663871362805e-05 -->grad_value: 1.987215000553988e-06 

INFO:root:-->name: lstm.bias_ih_l0 -->grad_requirs: True --weight -0.011691028252243996 -->grad_value: 0.0005940437549725175 

INFO:root:-->name: lstm.bias_hh_l0 -->grad_requirs: True --weight -0.009806161746382713 -->grad_value: 0.0005940437549725175 

INFO:root:-->name: lstm.weight_ih_l0_reverse -->grad_requirs: True --weight 0.0005512728821486235 -->grad_value: 0.0007477726321667433 

INFO:root:-->name: lstm.weight_hh_l0_reverse -->grad_requirs: True --weight -2.0616964320652187e-05 -->grad_value: 2.275753558933502e-06 

INFO:root:-->name: lstm.bias_ih_l0_reverse -->grad_requirs: True --weight -0.002702039200812578 -->grad_value: -0.0021463376469910145 

INFO:root:-->name: lstm.bias_hh_l0_reverse -->grad_requirs: True --weight -0.0037121018394827843 -->grad_value: -0.0021463376469910145 

INFO:root:-->name: lstm.weight_ih_l1 -->grad_requirs: True --weight 0.00046968788956291974 -->grad_value: 1.9119072021567263e-05 

INFO:root:-->name: lstm.weight_hh_l1 -->grad_requirs: True --weight -0.000309967203065753 -->grad_value: 6.383014260791242e-05 

INFO:root:-->name: lstm.bias_ih_l1 -->grad_requirs: True --weight -0.004660589620471001 -->grad_value: -0.010259311646223068 

INFO:root:-->name: lstm.bias_hh_l1 -->grad_requirs: True --weight -0.007270737551152706 -->grad_value: -0.010259311646223068 

INFO:root:-->name: lstm.weight_ih_l1_reverse -->grad_requirs: True --weight 0.0005389369907788932 -->grad_value: 1.1503866517159622e-05 

INFO:root:-->name: lstm.weight_hh_l1_reverse -->grad_requirs: True --weight -1.476364559493959e-05 -->grad_value: 0.0 

INFO:root:-->name: lstm.bias_ih_l1_reverse -->grad_requirs: True --weight -0.005187952425330877 -->grad_value: -0.005631891079246998 

INFO:root:-->name: lstm.bias_hh_l1_reverse -->grad_requirs: True --weight -0.00411349069327116 -->grad_value: -0.005631891079246998 

INFO:root:-->name: linear.weight -->grad_requirs: True --weight -0.001418228610418737 -->grad_value: 0.04317281022667885 

INFO:root:-->name: linear.bias -->grad_requirs: True --weight 0.01843859627842903 -->grad_value: -12.994577407836914 

INFO:root:
 ** Round 5 : Batch size = 25 , avg loss = 0.7244069337844848

INFO:root:-->name: normalization.weight -->grad_requirs: True --weight 0.9792694449424744 -->grad_value: 0.04715200886130333 

INFO:root:-->name: normalization.bias -->grad_requirs: True --weight -0.000385945662856102 -->grad_value: 0.22480832040309906 

INFO:root:-->name: lstm.weight_ih_l0 -->grad_requirs: True --weight 0.0012430117931216955 -->grad_value: -8.014107879716903e-05 

INFO:root:-->name: lstm.weight_hh_l0 -->grad_requirs: True --weight 3.1030940590426326e-05 -->grad_value: 1.018205875880085e-05 

INFO:root:-->name: lstm.bias_ih_l0 -->grad_requirs: True --weight -0.011692642234265804 -->grad_value: 0.0008833177853375673 

INFO:root:-->name: lstm.bias_hh_l0 -->grad_requirs: True --weight -0.009807774797081947 -->grad_value: 0.0008833177853375673 

INFO:root:-->name: lstm.weight_ih_l0_reverse -->grad_requirs: True --weight 0.0005512782372534275 -->grad_value: 0.001520714838989079 

INFO:root:-->name: lstm.weight_hh_l0_reverse -->grad_requirs: True --weight -2.0572661014739424e-05 -->grad_value: 3.0548412723874208e-06 

INFO:root:-->name: lstm.bias_ih_l0_reverse -->grad_requirs: True --weight -0.0027027439791709185 -->grad_value: -0.00024896225659176707 

INFO:root:-->name: lstm.bias_hh_l0_reverse -->grad_requirs: True --weight -0.003712806850671768 -->grad_value: -0.00024896225659176707 

INFO:root:-->name: lstm.weight_ih_l1 -->grad_requirs: True --weight 0.00046974816359579563 -->grad_value: 1.8225624444312416e-05 

INFO:root:-->name: lstm.weight_hh_l1 -->grad_requirs: True --weight -0.0003099957248196006 -->grad_value: 5.388038698583841e-05 

INFO:root:-->name: lstm.bias_ih_l1 -->grad_requirs: True --weight -0.004661282990127802 -->grad_value: -0.00895683653652668 

INFO:root:-->name: lstm.bias_hh_l1 -->grad_requirs: True --weight -0.00727143045514822 -->grad_value: -0.00895683653652668 

INFO:root:-->name: lstm.weight_ih_l1_reverse -->grad_requirs: True --weight 0.0005389666766859591 -->grad_value: 1.0379391824244522e-05 

INFO:root:-->name: lstm.weight_hh_l1_reverse -->grad_requirs: True --weight -1.476364559493959e-05 -->grad_value: 0.0 

INFO:root:-->name: lstm.bias_ih_l1_reverse -->grad_requirs: True --weight -0.005188334733247757 -->grad_value: -0.003546813502907753 

INFO:root:-->name: lstm.bias_hh_l1_reverse -->grad_requirs: True --weight -0.00411387300118804 -->grad_value: -0.003546813502907753 

INFO:root:-->name: linear.weight -->grad_requirs: True --weight -0.0014181024162098765 -->grad_value: 0.04298834875226021 

INFO:root:-->name: linear.bias -->grad_requirs: True --weight 0.018445279449224472 -->grad_value: -11.844635009765625 

